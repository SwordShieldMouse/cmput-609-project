\documentclass{article}

\usepackage[english]{babel}
\usepackage[square, numbers]{natbib}
\usepackage{url}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{gensymb}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage[margin=1.0in]{geometry}
\usepackage{algorithm, algpseudocode}
\usepackage{bm}
\usepackage{caption}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{tikz}
\usetikzlibrary{shapes}

\usepackage[title]{appendix}
\usepackage{cleveref}

\pgfplotsset{height=8cm, width=15cm,compat=1.9}

\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Ex}{\mathbb{E}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\title{CMPUT 609 Course Project}
\author{Alan Chan}
\date{}

\begin{document}
  \maketitle

  \section{Introduction}
  Policy gradient methods in Reinforcement Learning (RL) are a way of learning a policy in a Markov Decision Process (MDP) so as to maximize an objective function, usually the expected reward. 

  \section{Methods}

  \section{Results & Discussion}

  \section{Conclusion}
\end{document}
